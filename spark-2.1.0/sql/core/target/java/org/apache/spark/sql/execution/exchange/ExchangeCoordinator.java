package org.apache.spark.sql.execution.exchange;
/**
 * A coordinator used to determines how we shuffle data between stages generated by Spark SQL.
 * Right now, the work of this coordinator is to determine the number of post-shuffle partitions
 * for a stage that needs to fetch shuffle data from one or multiple stages.
 * <p>
 * A coordinator is constructed with three parameters, <code>numExchanges</code>,
 * <code>targetPostShuffleInputSize</code>, and <code>minNumPostShufflePartitions</code>.
 *  - <code>numExchanges</code> is used to indicated that how many {@link ShuffleExchange}s that will be registered
 *    to this coordinator. So, when we start to do any actual work, we have a way to make sure that
 *    we have got expected number of {@link ShuffleExchange}s.
 *  - <code>targetPostShuffleInputSize</code> is the targeted size of a post-shuffle partition's
 *    input data size. With this parameter, we can estimate the number of post-shuffle partitions.
 *    This parameter is configured through
 *    <code>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code>.
 *  - <code>minNumPostShufflePartitions</code> is an optional parameter. If it is defined, this coordinator
 *    will try to make sure that there are at least <code>minNumPostShufflePartitions</code> post-shuffle
 *    partitions.
 * <p>
 * The workflow of this coordinator is described as follows:
 *  - Before the execution of a {@link SparkPlan}, for a {@link ShuffleExchange} operator,
 *    if an {@link ExchangeCoordinator} is assigned to it, it registers itself to this coordinator.
 *    This happens in the <code>doPrepare</code> method.
 *  - Once we start to execute a physical plan, a {@link ShuffleExchange} registered to this
 *    coordinator will call <code>postShuffleRDD</code> to get its corresponding post-shuffle
 *    {@link ShuffledRowRDD}.
 *    If this coordinator has made the decision on how to shuffle data, this {@link ShuffleExchange}
 *    will immediately get its corresponding post-shuffle {@link ShuffledRowRDD}.
 *  - If this coordinator has not made the decision on how to shuffle data, it will ask those
 *    registered {@link ShuffleExchange}s to submit their pre-shuffle stages. Then, based on the
 *    size statistics of pre-shuffle partitions, this coordinator will determine the number of
 *    post-shuffle partitions and pack multiple pre-shuffle partitions with continuous indices
 *    to a single post-shuffle partition whenever necessary.
 *  - Finally, this coordinator will create post-shuffle {@link ShuffledRowRDD}s for all registered
 *    {@link ShuffleExchange}s. So, when a {@link ShuffleExchange} calls <code>postShuffleRDD</code>, this coordinator
 *    can lookup the corresponding {@link RDD}.
 * <p>
 * The strategy used to determine the number of post-shuffle partitions is described as follows.
 * To determine the number of post-shuffle partitions, we have a target input size for a
 * post-shuffle partition. Once we have size statistics of pre-shuffle partitions from stages
 * corresponding to the registered {@link ShuffleExchange}s, we will do a pass of those statistics and
 * pack pre-shuffle partitions with continuous indices to a single post-shuffle partition until
 * the size of a post-shuffle partition is equal or greater than the target size.
 * For example, we have two stages with the following pre-shuffle partition size statistics:
 * stage 1: [100 MB, 20 MB, 100 MB, 10MB, 30 MB]
 * stage 2: [10 MB,  10 MB, 70 MB,  5 MB, 5 MB]
 * assuming the target input size is 128 MB, we will have three post-shuffle partitions,
 * which are:
 *  - post-shuffle partition 0: pre-shuffle partition 0 and 1
 *  - post-shuffle partition 1: pre-shuffle partition 2
 *  - post-shuffle partition 2: pre-shuffle partition 3 and 4
 */
public  class ExchangeCoordinator implements org.apache.spark.internal.Logging {
  static protected  java.lang.String logName ()  { throw new RuntimeException(); }
  static protected  org.slf4j.Logger log ()  { throw new RuntimeException(); }
  static protected  void logInfo (scala.Function0<java.lang.String> msg)  { throw new RuntimeException(); }
  static protected  void logDebug (scala.Function0<java.lang.String> msg)  { throw new RuntimeException(); }
  static protected  void logTrace (scala.Function0<java.lang.String> msg)  { throw new RuntimeException(); }
  static protected  void logWarning (scala.Function0<java.lang.String> msg)  { throw new RuntimeException(); }
  static protected  void logError (scala.Function0<java.lang.String> msg)  { throw new RuntimeException(); }
  static protected  void logInfo (scala.Function0<java.lang.String> msg, java.lang.Throwable throwable)  { throw new RuntimeException(); }
  static protected  void logDebug (scala.Function0<java.lang.String> msg, java.lang.Throwable throwable)  { throw new RuntimeException(); }
  static protected  void logTrace (scala.Function0<java.lang.String> msg, java.lang.Throwable throwable)  { throw new RuntimeException(); }
  static protected  void logWarning (scala.Function0<java.lang.String> msg, java.lang.Throwable throwable)  { throw new RuntimeException(); }
  static protected  void logError (scala.Function0<java.lang.String> msg, java.lang.Throwable throwable)  { throw new RuntimeException(); }
  static protected  boolean isTraceEnabled ()  { throw new RuntimeException(); }
  static protected  void initializeLogIfNecessary (boolean isInterpreter)  { throw new RuntimeException(); }
  public   ExchangeCoordinator (int numExchanges, long advisoryTargetPostShuffleInputSize, scala.Option<java.lang.Object> minNumPostShufflePartitions)  { throw new RuntimeException(); }
  /**
   * Registers a {@link ShuffleExchange} operator to this coordinator. This method is only allowed to
   * be called in the <code>doPrepare</code> method of a {@link ShuffleExchange} operator.
   * @param exchange (undocumented)
   */
  public  void registerExchange (org.apache.spark.sql.execution.exchange.ShuffleExchange exchange)  { throw new RuntimeException(); }
  public  boolean isEstimated ()  { throw new RuntimeException(); }
  /**
   * Estimates partition start indices for post-shuffle partitions based on
   * mapOutputStatistics provided by all pre-shuffle stages.
   * @param mapOutputStatistics (undocumented)
   * @return (undocumented)
   */
  public  int[] estimatePartitionStartIndices (org.apache.spark.MapOutputStatistics[] mapOutputStatistics)  { throw new RuntimeException(); }
  private  void doEstimationIfNecessary ()  { throw new RuntimeException(); }
  public  org.apache.spark.sql.execution.ShuffledRowRDD postShuffleRDD (org.apache.spark.sql.execution.exchange.ShuffleExchange exchange)  { throw new RuntimeException(); }
  public  java.lang.String toString ()  { throw new RuntimeException(); }
}
